---
title: "Concepts for QA and AM"
subtitle: "Quality Assurance and Automation"
author: "Peter von Rohr"
date: "`r Sys.Date()`"
output:
  bookdown::tufte_html2: default
  bookdown::tufte_handout2:
    citation_package: natbib
    latex_engine: xelatex
  bookdown::tufte_book2:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
# conversion hook
knitr::knit_hooks$set(hook_convert_odg = rmddochelper::hook_convert_odg)
```


# Disclaimer
This document gives an outline of some concepts proposed for quality assurance (QA) and automation (AM). Based on the abbreviations for quality assurance and automation, the project is named `QAAM`. In a first step, we are focusing on the QA-part of the project. The automation-part should follow as an implicit by-product.


# Concept of Quality Assurance (QA) in a Data Analysis (DA)
When looking at quality assurance in a data analysis it appears to be useful to differentiate between the time period^[See the diagram  (Figure \@ref(fig:da-period)) below. The horizontal axis corresponds to time and the vertical arrow points to the timepoint when the data analysis is done.]

*  __before__ and 
*  __after__ 

the analysis has been done. 


```{r da-period, echo=FALSE, hook_convert_odg=TRUE, fig_path="../odg", fig.cap="Periods Of A Data Analysis", fig.fullwidth=TRUE}
#rmddochelper::use_odg_graphic(ps_path = '../odg/da-period.odg')
knitr::include_graphics(path = "../odg/da-period.png")
```

In an ideal case, the time period before the analysis, should be used for planning the analysis. Depending on whether the data comes out of a designed experiment or whether the data is collected for some other reason, the work that is done during the planning phase can look different. An experiment should be designed such that the questions of interest can be answered within the budget constraints. When the data is collected for other purposes, the data analysis can still be planned, e.g. by comparing different statistical methods based on previously collected or based on simulated data.

In the context of livestock breeding, the data is often collected for other purposes. That means the primary purpose of the collected data is not the use in a breeding program. Hence the data that are analyzed in this context are mostly given and the underlying structure of the data cannot be changed. Hence, the planning phase is reduced to comparing different statistical methods for using the given data in an optimal way. In most cases similar data of different countries are available or are described in publications and with results from such previous studies, we can check whether the chosen statistical method is suitable.

Once the data analysis is done, the results of the analysis can be inspected and compared to our expectations from the planning phase or to other results based on the same kind of data. These comparisons might reveal unexpected features of the results. In such a situation, it is very important that we can trust these results. But results of a data analysis can only be trustworthy, when the analysis producing the results fullfills certain quality standards. At this point it is important to note that we must have some measure or some criterion that allows us to quantify the quality of a data analysis. 

The concepts developed here are more concerned about how to establish measures of quality for the period after the analysis has been done. Everything related to the planning phase is postponed to a different part of the project. 


# Unifying Criterion
The essential criterion of a data analysis that allows us to make any statements about the quality of that analysis is __reproducibility__^[Have a look at the appendix of this document for a definition of reproducibility]. It must be emphasized that reproducibility on its own does not guarantee any level of quality, but without reproducibility, it is impossible to assess the quality of a data analysis.

## How to ensure reproducibility {#ensurerepro}
Nowadays all data analyses are done using a certain software program. From now on, we call such programs `statistical computing system`^[Examples of such systems are MS Excel, SPSS, SAS, S-Plus, Python, R and many more.]. These systems can be used either  interactively or they can be driven by specific programming languages.  When using such a system interactively, the user enters commands or navigates in GUI-based^[where GUI stands for Graphical User Interface] menus. Data analyses which are done interactively, cannot be made reproducible or are very difficult to be made reproducible^[We shall see later a weaker form of reproducibility which can be postulated for such analyses].

Most statistical computing systems also contain domain specific programming languages. This means that scripts or programs which correspond to sequences of statements that are collected in a file^[which are called the script or the program] are executed in the statistical computing system. The results produced by the data-analysis program must  be collected in a result-file from which it is clear what the name of the program is and when the program was run to produce the given results^[more detailed information such as the version of the statistical computing system, any additional software needed, the operating system and the name of the machine can be helpful and are therefore desirable to be included in a result file]. 

Paradigms such as `literate programming` [@Knuth1984] mix programming logic with explaining text. Such approaches are very helpful in introducing reproducibility properties for a given data analysis.


## Weak form of reproducibility
When using a statistical computing system interactively or when performing the data analysis based on a given recipe, we can still obtain a weaker form of reproducibility. This can be achieved by collecting all output that is produced in a protocol document. This protocol lists the steps in the recipe with the associated output. Including the filename of the recipe and the name of the input data and the date when the analysis was performed, we can obtain a weaker form of the above described reproducibility property.


## Ten rules for reproducibility
@SNTH2013 gave ten simple rules that should be followed to reach the goal of reproducibility in computational research. 

__Rule 1__: For Every Result, Keep Track of How It Was Produced

* single commands, scripts, programs in executable script forms

__Rule 2__: Avoid Manual Data Manipulation Steps

* inefficient and error-prone not easily reproducible

__Rule 3__: Archive the Exact Versions of All External Programs Used

* it is not always trivial to get hold of a program in anything but the current version
* to ensure future availability, the only viable solution may then be to store a full virtual machine image

__Rule 4__: Version Control All Custom Scripts

* to track evolution of code, use a version control system e.g. Git

__Rule 5__: Record All Intermediate Results, When Possible in Standardized Formats

* if enough storage

__Rule 6__: For Analyses That Include Randomness, Note Underlying Random Seeds

* allows exact results reproducibility

__Rule 7__: Always Store Raw Data behind Plots

* for example R code and the dataframe used as input

__Rule 8__: Generate Hierarchical Analysis Output, Allowing Layers of Increasing Detail to Be Inspected

* if storage allows it, keep track of all intermediate steps that led to the final results

__Rule 9__: Connect Textual Statements to Underlying Results

* within the text description always add links to scripts/data that shaped you logical deductions

__Rule 10__: Provide Public Access to Scripts, Runs, and Results

* for example link to your github account


# How to use reproducibility for quality assurance 
Based on the definition of reproducibility, we want to look at the results of a data analysis and be able to determine when the given results were produced, which input data were used and which program has generated those specific results. This requires every program to write the name of the program, the name of the input data file and the time of program execution to the result file. As already mentioned, further information about the analysis such as the machine name, the operating system, the version of the program and any additional software that were used can be very helpful. 

Using the information in the resultfile as described in the previous paragraph, we can determine when, based on which input data and by which program a certain result file was generated. Because programs can change over time, it is important to use a version control system^[such as git, svn or cvs] for all the programs that are used in a data analysis. Only with a version control system, it is possible to be able to re-create a version of a program at a certain time point. 

In general input data are too big to be included in a version control system. Furthermore, in most cases input data just grow over time, but they are very rarely changed in a different way. Hence the functionality of a version control system is not really needed for input data. It is sufficient to move input data into an archive where they can be restored in case that a certain analysis has to be reproduced. 

Using all this information that characterizes a given data analysis, we are able to reproduce a certain analysis at any given point in time. This is particulary useful if a problem is found in a given data analysis. Being able to reproduce a certain data analysis, it is much easier to search for the causes of the observed problem.


# Techniques And Tools For Reproducible Data Analysis
## Templates
In the previous section, the required information to be included in a data analysis result file was listed. Because each program must write this information into the resultfile and this information is more or less constant, the use of prepared program templates can be very helpful. The template makes sure that the required information is automatically written to the result file. Most programs used for data analyses are written in Bash, Fortran, C/C++, Python or R. For each of these languages templates will be provided that satisfy the described requirement that make any given data analysis reproducible. 


## Literate Programming
As mentioned in section \@ref(ensurerepro), `literate programming` is a paradigm developed by [@Knuth1984]. Literate programming mixes programming logic and explaining text. All information, i.e., program code and the explanatory text is contained in a single source file. Such a source file can be translated into a readable document or compilable or executable code can be extracted from the source file. Quite a number of literate programming systems exist. For R and LaTeX there is `Sweave` and for R and Markdown there is `rmarkdown` and `knitr`. For Python and Julia there are IPython notebooks. 

Combining code and text together with results allows to create reproducible data analysis reports very easily. 


## Version Control
Programs change over time. In order to be able to reproduce any results in the post-analysis time period, it is important to be able to go back or to retrieve the version of the program as it existed at the time-point of the analysis. Changes to programming sources can be recorded with version control (VC) systems^[Examples of VC systems are CVS, SVN and git.]. The usage of a VC system always starts by creating or initializing a repository. Then the material that is to be administered with the VC system must be imported into the repository. As soon as changes are applied to the sources they have to be committed to the repository. Any collaborator that is also interested in working on the same source, has to synchronize her or his local copy with the newer version on the repository. Older versions of the source code can be retrieved very easily using the functionality provided by the VC system.  

Due to the way how a VC system works, the source code has to be in text files with an open format. Only when the sources are organized in an open format, then VC system can track and record changes reliably. Files in binary and or closed formats (such as .xlsx, ...) are very difficult to be administered in a VC system.


## Docker Containers for Reproducible Research
[Docker](https://www.docker.com) is a tool to easily deploy and run applications by using containers. Containers make it easy for developers to package the required parts such as libraries and other dependencies for any given application. 


# Appendix

## Definition of Reproducibility
```{r reprodef, echo=FALSE, hook_convert_odg=TRUE, fig_path="../odg", fig.margin = TRUE, fig.cap="Definition of Reproducibility"}
#rmddochelper::use_odg_graphic(ps_path = "../odg/reprodef.odg")
knitr::include_graphics(path = "../odg/reprodef.png")
```

A data analysis is _reproducible_ if a data analyst which is not the same person as the one who has done the data analysis in the first place, can reproduce the results of the analysis with the same input data and the same programs as the analysis was done by the original data analyst. 

This other person that tries to reproduce any given data analysis from the above definition of reproducibility might also be future "you". In that case reproducibility yields some added benefit for the data analyst her- or himself.


@Stevens2017 gives a distinction between reproducibility and replication.


# Further Resources
This chapter mentions a few resources concerning the concept of reproducibility

## Scientific Research
@FV2017 is a good resource providing a bird's eye view of the developments in open scientific research (OSR). OSR is also strongly based on reproducibility of scientific results. 

## Reproducibility Crisis in Research
The term _reproducibility crisis_ or replication crisis has its own [Wikipedia entry](https://en.wikipedia.org/wiki/Replication_crisis). This term summarizes the problem of researchers to reproduce any published scientific results. 


## Reproducibility and Software
The whole process shown in Figrue \@ref(fig:reprodef) is completely software-driven. Hence questions related to QA of the whole process do also relate to issues of QA in software-design. [@BF2004] (Chapter 10) is one of the many references on software quality. One aspect that appears to be relevant in our context is the software quality aspect that is related to correctness of a software program. The term correctnes here is defined as "the degree to which a software product meets established requirements"^[Often we do not have explicit requirements allowing for a correctness evaluation. In most cases, implicit requirements can be used to get a correctness estimate].  


```{r bib, include=FALSE}
s_bib_file <- "skeleton.bib"
vec_bref <- c(bibentry(
  bibtype = "Book",
  title = "R for Data Science",
  author = c(as.person("H. Wickham"), as.person("G. Grolemund")),
  year = "2017",
  publisher = "O'Reilly",
  url = "http://r4ds.had.co.nz/",
  key = "WG2017"
),
bibentry(
  bibtype = "Book",
  title = "Guide to the Software Engineering Body of Knowledge (SWEBOK)",
  editor = c(as.person("P. Boruque"), as.person("R. E. Fairley")),
  year = "2004",
  publisher = "IEEE",
  url = "https://www.computer.org/web/swebok/index",
  key = "BF2004"
),
bibentry(
  bibtype = "Book",
  title = "Open Science, Open Data, Open Source",
  author = c(as.person("P. L. Fernandes"), as.person("R. A. Vos")),
  publisher = "GitBook",
  url = "https://pfern.github.io/OSODOS/gitbook/",
  year = 2017,
  key = "FV2017"),
bibentry(
  bibtype = "Article",
  title = "Replicability and Reproducibility in Comparative Psychology",
  author = as.person("J.R. Stevens"),
  journal = "Front Psychol",
  year = 2017,
  volume = 8,
  pages = "862",
  key = "Stevens2017"),
bibentry(
  bibtype = "Article",
  title   = "Ten Simple Rules for Reproducible Computational Research",
  author  = c(as.person("GK Sandve"), 
              as.person("A Nekrutenko"),
              as.person("J Taylor"),
              as.person("E Hovig")),
  journal = "PLoS Comput Biol",
  year    = 2013,
  volume  = 9,
  pages   = "e1003285",
  doi     = "https://doi.org/10.1371/journal.pcbi.1003285",
  key     = "SNTH2013"),
bibentry(
  bibtype = "Article",
  title   = "Literate Programming",
  author  = as.person("D E Knuth"),
  journal = "The Computer Journal. British Computer Society",
  year    = 1984,
  volume  = 27,
  pages   = "97--111",
  doi     = "doi:10.1093/comjnl/27.2.97",
  key     = "Knuth1984"
)
)

### # output to bib file
rmddochelper::write_bib(pvec_bref = vec_bref, ps_bibfile = s_bib_file)

# create a bib file for the R packages used in this document
# knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```

  
