---
title: "Concepts for QA and AM"
subtitle: "Quality Assurance and Automation"
author: "Peter von Rohr"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

```{r, echo=FALSE}
knitr::opts_chunk$set(odg.conv=rmddochelper::odg.graphics.conv.hook)
```


# Disclaimer
This document gives an outline of some concepts proposed in the area of quality assurance (QA) and automation (AM)


# Introduction
According to R for Data Science [@WG2017], the following model of a data science project can be proposed. 

```{r DsProjectModel,echo=FALSE, odg.conv=TRUE, odg.path="odg", odg.out.dir="png", odg.graph.cache=TRUE, fig.cap="Data Science Project Model", fig.margin=TRUE}
knitr::include_graphics(path = "png/DsProjectModel.png")
```

The model as defined above starts with importing the data into a statistics tool such as R. Hence, we assume that the data is available in a file in a pre-defined format or in a database in a specified set of tables. For the design of the routine analyses that will be discussed later, the process of exporting data from a given database will also be part of the model. This data export process can be thought of as a requirement of the above shown data science project model.


## Componenents
The above shown components can be explained as follows

* __Import__: data must be imported first from a file^[When data is imported from file, a data-export process might be required to extract the data. Alternatively, it would be more convenient to directly transfer the data from the database into the statistics system.] or a database
* __Tidy__: data is cleaned, verified and converted into a standard format, where each data record is on one row and each variable is represented by one column. 
* __Transform__: after tidying certain variables are transformed. Here transformation also includes the computation of new variables from existing ones
* __Visualise__: shows properties of data that are not necessarily expected, but visualisations do not scale
* __Model__: are used to answer concrete questions and do scale well
* __Communicate__: presentation of results


# Routine Data Analysis Process
In the context of the proposed concepts, the term _routine_ is defined as a data analysis task that is done using the same data science model components on a dataset that is growing over time. Hence some of the iterative components between transformation, visualisation and modelling might be replaced by a given path between the mentioned components that was determined in data science project that is done as a preparatory study for the routine data analysis. 


## Preparatory Study
Before, we can start a new data analysis that is planned to be done according to a routine schedule, we have to conduct a preparatory study that investigates a certain number of research questions. Such a preparatory study can be done according to the above given Data Science Project Model. The results of this preparatory study are not only the outcomes of the statistical analyses, but we also save the whole process of how we proceed from the input of the raw data to the statistical results. This process is defined as path between the different data science model components and it will be used in the routine data analysis. 

An example of finding such a routine process is shown below (see Figure \ref{fig:find_routine_process}).

```{r find_routine_process, echo=FALSE, fig.cap="\\label{fig:find_routine_process}Create Routine Analysis From Results Of Preparatory Study", fig.height=3}
s_graphics_file <- "odg/find_routine_process.odg"
if (!file.exists(s_graphics_file)){
  rmddochelper:::odg_draft(file = s_graphics_file, package = "rmddochelper")
}
s_include_file <- gsub("odg", "png", s_graphics_file, fixed = TRUE)
knitr::include_graphics(path = s_include_file)
```


## Routine Design
The above shown result from the preparatory study can now be applied in a given routine schedule plan. The repetition of the routine data analysis over time with data that increases over time leads to the final process of a routine data analysis process. This final process is shown below.

```{r design_routine, echo=FALSE, fig.cap="\\label{fig:design_routine}Routine Data Analysis Process", fig.height=3}
s_design_routine_graphics_file <- "odg/design_routine.odg"
if (!file.exists(s_design_routine_graphics_file)){
  rmddochelper:::odg_draft(file = s_design_routine_graphics_file, package = "rmddochelper")
  system(paste(rmddochelper::get_odg_prog_path(), s_design_routine_graphics_file))
}
s_design_routine_include_file <- gsub("odg", "png", s_design_routine_graphics_file, fixed = TRUE)
knitr::include_graphics(path = s_design_routine_include_file)
```


# Concept of Quality Assurance (QA) in a Data Analysis
The most important criterion of a data analysis which decides whether any statement about the quality of a data analysis can be made is __reproducibility__^[The term 'reprducibility' is to be understood in the context of a data analysis.]. It has to be emphasized that reproducibility alone does not guarantee a high level of quality of any data analysis. But without reproducibility, the quality of a data analysis  is impossible or at best very difficult to assess. 


## Definition of Reproducibility
A data analysis is _reproducible_ if a data analyst which is not the same person the one who has done the data analysis in the first place, can reproduce the results of the analysis with the same input data and the same programs as the analysis was done by the original data analyst. 


## How to ensure reproducibility
Nowadays any data analysis is done with a statistical computing system. These systems can be used interactively where the user enters commands or navigates in a GUI-based^[where GUI stands for Graphical User Interface] menu. Data analyses which are done in such an interactive mode cannot be made reproducible or are difficult to make reproducible. We shall see later a weaker form of reproducibility which can be postulated for such analyses.

Most statistical computing systmes can also be run with scripts or programs where sequences of statements are collected in a file^[which are called the script or the program] and these statements can then be run against the statistical computing system. The results produced by the data-analysis program must then be collected in a new result-file from which it is clear what the name of the program is and when the program was run to produce the given results^[more detailed information such as the version of the statistical computing system, any additional software needed, the operating system and the name of the machine can be helpful and are therefore desirable to be included in a result file]. 



## Measures of Quality Assurance
The whole process shown in Figrue \ref{fig:design_routine} is completely software-driven. Hence questions related to QA of the whole process do also relate to issues of QA in software-design. 


```{r bib, include=FALSE}
s_bib_file <- "skeleton.bib"
vec_bref <- c(bibentry(
  bibtype = "Book",
  title = "R for Data Science",
  author = c(as.person("H. Wickham"), as.person("G. Grolemund")),
  year = "2017",
  publisher = "O'Reilly",
  url = "http://r4ds.had.co.nz/",
  key = "WG2017"
))

### # output to bib file
rmddochelper::write_bib(pvec_bref = vec_bref, ps_bibfile = s_bib_file)

# create a bib file for the R packages used in this document
# knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```


